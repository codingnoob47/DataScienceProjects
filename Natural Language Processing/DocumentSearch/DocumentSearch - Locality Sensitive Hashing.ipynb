{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents : \n",
    "\n",
    " - Implementing a Pipeline to Process Documents and create a vocabulary specific to the data considered\n",
    " - Using Pretrained Glove embeddings for faster lookup\n",
    " - Implementing hash tables using Locality Sensitive Hashing\n",
    " - Retrieved documents with simlar embeddings using LSH and the lookup architecture developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import scipy\n",
    "import sklearn\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    \"\"\"\n",
    "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
    "    Check out the files this function takes in your workspace.\n",
    "    \"\"\"\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        # indexing into the rows.\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    '''\n",
    "    # you have to set this variable to the true label.\n",
    "    cos = -10\n",
    "    dot = np.dot(A, B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "\n",
    "    return cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = pd.read_csv('review.csv')\n",
    "    data = data[['reviews.text','reviews.title']]\n",
    "    data['Title_Review'] = data['reviews.text']+' ' + data['reviews.title']\n",
    "    data = data.iloc[:,-1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    tokens = data.split()\n",
    "    \n",
    "    re_punct = re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    \n",
    "    cleaned_text = [re_punct.sub('',w) for w in tokens]\n",
    "    \n",
    "    cleaned_text = [word for word in cleaned_text if word.isalpha()]\n",
    "    \n",
    "    stop_words =  stopwords.words('english')\n",
    "    cleaned_text = [word for word in cleaned_text if word not in stop_words]\n",
    "    \n",
    "    cleaned_text = [word for word in cleaned_text if len(word) > 1]\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(vocabulary):\n",
    "    data = load_data()\n",
    "    for item in list(data):\n",
    "        clean_text = clean_data(str(item))\n",
    "        vocabulary.update(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vocabulary(min_occurences,vocabulary):\n",
    "    tokens = [word for word,count in vocabulary.items() if count >= min_occurences]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(word_list, file_name):\n",
    "    string = '\\n'.join(word_list)\n",
    "    file = open(file_name,'w')\n",
    "    file.write(string)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Counter()\n",
    "min_occurences = 2\n",
    "add_doc_to_vocab(vocabulary)\n",
    "tokens = process_vocabulary(min_occurences,vocabulary)\n",
    "save_list(tokens, 'vocabulary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### >>> Checkpoint :  At this point we have a vocabulary of all the words present in the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shekhartanwar/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1868"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_len = [len(str(item).split()) for item in load_data()]\n",
    "max_len = max(review_len)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(File,'r')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "File = 'glove.6B.300d.txt'\n",
    "gloveModel = loadGloveModel(File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(data):\n",
    "    \n",
    "    data = str(data)\n",
    "    cleaned_text = re.sub(r'\\$\\w*', '', data)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    cleaned_text = re.sub(r'^RT[\\s]+', '', data)\n",
    "    # remove hyperlinks\n",
    "    cleaned_text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', data)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    cleaned_text = re.sub(r'#', '', data)\n",
    "    \n",
    "    tokens = data.split()\n",
    "    \n",
    "    re_punct = re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    \n",
    "    cleaned_text = [re_punct.sub('',w) for w in tokens]\n",
    "    \n",
    "    \n",
    "    cleaned_text = [word for word in cleaned_text if word.isalpha()]\n",
    "    \n",
    "\n",
    "    cleaned_text = [word for word in cleaned_text if len(word) > 1]\n",
    "    \n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(doc, gloveModel):\n",
    "    \n",
    "    word_list = process_doc(doc)\n",
    "    \n",
    "    document_embedding = np.zeros(300)\n",
    "    \n",
    "    for word in word_list:\n",
    "        document_embedding = document_embedding + gloveModel.get(word,0)\n",
    "    return document_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for One Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.0332600e-01,  7.5041400e-01, -7.1783900e-02, -1.9265000e-02,\n",
       "        4.7959300e-01,  6.0707500e-01, -8.1272000e-02,  6.5340000e-02,\n",
       "        2.3001400e-01, -8.5065000e+00,  1.2674200e+00, -6.1185700e-01,\n",
       "       -4.1832900e-01,  5.1154200e-01, -4.2860260e-01,  2.0099520e-01,\n",
       "       -5.2921300e-01, -2.7080200e-01, -4.2990000e-01, -1.6857600e-01,\n",
       "        3.3753000e-01,  2.2448900e+00,  1.0648700e+00, -2.2478900e-01,\n",
       "       -1.5229500e+00,  2.6710000e-01, -3.7438000e-02, -1.3987400e+00,\n",
       "       -3.9200800e-01,  9.8495200e-01,  6.3252100e-01,  1.5341100e+00,\n",
       "       -1.4177600e+00, -7.5303900e-01, -4.0441400e+00,  1.8751000e-01,\n",
       "       -3.1893250e-01,  3.1450600e-01,  9.9340000e-02, -5.7609200e-01,\n",
       "        1.0635300e-01, -1.2342800e+00, -5.9192700e-01,  3.3345500e-01,\n",
       "       -6.9512000e-02,  7.5312100e-01,  2.9538100e-01,  4.6013000e-01,\n",
       "       -7.2189000e-01,  8.4915900e-01,  8.3845000e-01, -1.4046400e+00,\n",
       "        1.7329400e-01, -1.2066000e-01, -8.5324900e-01,  1.3768000e+00,\n",
       "        6.1117000e-01,  7.2220000e-01,  6.5586500e-01,  2.8676120e-01,\n",
       "        1.2924570e+00,  7.3706300e-01,  7.8019900e-01,  1.7881700e+00,\n",
       "       -5.5643600e-01, -1.5122900e+00,  4.6134200e-01,  4.5195370e-01,\n",
       "       -4.0358500e-01,  1.0196800e-01,  5.5135500e-01, -5.0720000e-02,\n",
       "        4.2415000e-01,  1.6772200e+00,  2.7681500e-01,  1.9989100e-02,\n",
       "        6.1552500e-01,  7.0895700e-01, -8.1924200e-01, -7.6192600e-01,\n",
       "       -1.5148100e+00, -9.3359720e-02,  1.6015500e+00, -5.3997400e-01,\n",
       "        3.7853000e-01,  6.5291900e-01, -9.6497500e-01,  1.5238900e+00,\n",
       "        9.7056000e-02,  6.6415300e-01, -1.6974300e+00,  2.0160700e+00,\n",
       "       -1.6357800e+00, -3.6252000e-01,  4.2585000e-02,  6.6770000e-01,\n",
       "       -8.8309300e-01,  4.7482700e-01,  7.6085000e-01, -9.5667400e-01,\n",
       "        1.6291900e-01,  1.9517200e-01, -9.7011000e-02, -4.4640200e-01,\n",
       "       -1.6481000e-01,  5.5451100e-01,  1.3429900e+00,  1.0288810e+00,\n",
       "       -6.1257400e-01,  9.1666000e-01, -2.8434500e-01, -1.3907700e+00,\n",
       "       -3.7532600e-01, -1.0696990e+00,  1.1462500e-01,  8.5455000e-01,\n",
       "       -8.6038000e-01, -9.0577000e-02,  1.0606400e-01, -1.1799100e+00,\n",
       "        1.8276480e-01, -1.6085800e+00,  3.7505000e-02,  9.3323300e-01,\n",
       "        2.9012400e-01,  2.2764400e-01, -6.9810000e-01,  5.2639000e-01,\n",
       "       -4.9789000e-02,  1.0069200e-01,  5.3800000e-01, -5.9514400e-01,\n",
       "       -5.8596000e-02,  8.7949500e-01, -9.4547000e-01, -1.6290400e-01,\n",
       "       -5.9744600e-01, -2.6402700e-01, -5.6546100e-01,  1.0199600e+00,\n",
       "       -6.5164100e-01,  1.0868900e+00, -3.4340000e-02,  1.3468000e+00,\n",
       "       -1.5577580e+00, -2.0402400e-01,  1.0515000e-01,  2.2122200e-01,\n",
       "        3.1763000e-02,  3.1067800e-01, -1.1261300e+00, -7.7864600e-01,\n",
       "        5.1173000e-01,  5.5550000e-01,  3.6085700e-01,  4.8810000e-03,\n",
       "       -1.7604700e-01, -6.2592400e-01,  9.5007000e-01, -1.0569800e-01,\n",
       "        4.0533800e-01, -1.5133000e+00,  4.2289000e-02,  4.6521000e-01,\n",
       "        3.4635200e-01,  1.0119800e+00,  2.5330480e-01,  9.7190000e-02,\n",
       "        2.1722600e-01, -2.0585300e-01,  1.9933000e-02,  5.5068000e-02,\n",
       "       -3.1297300e+00, -3.4318710e-01,  4.7177300e-01, -1.4072500e-01,\n",
       "       -9.0119000e-01,  1.1571426e+00,  5.9945100e-01,  1.3411080e+00,\n",
       "       -3.0008700e-01, -1.7271000e-01,  3.7481490e-01, -1.6808010e-01,\n",
       "       -5.8314500e-01, -4.4337000e-01, -4.2746600e-01,  7.8318300e-01,\n",
       "        1.5637500e+00,  2.4785000e-01, -3.5050300e-01,  1.9636900e-01,\n",
       "        7.6857100e-01, -4.7120500e-01,  5.3701880e-01, -1.2353100e+00,\n",
       "        1.1870000e-01,  9.7867000e-01,  4.3821400e-01, -1.3884160e+00,\n",
       "        3.7659200e+00, -3.6773570e-01,  6.6655000e-01,  3.3070000e-01,\n",
       "        5.5010500e-01, -5.7439000e-01,  2.4166000e-01,  1.1114500e+00,\n",
       "       -1.0573400e-01, -3.6431000e-01, -1.5012800e-01, -1.0317400e-02,\n",
       "        6.2482500e-01, -2.3040600e-01,  1.0980000e-01,  3.9631000e-02,\n",
       "        5.7872000e-01,  1.3418200e+00,  1.6733550e-01,  3.4455000e-01,\n",
       "        1.0023490e+00,  2.4929000e-01, -1.9988440e-01, -2.3723000e-01,\n",
       "        1.1621500e+00,  1.7426900e-01,  6.6958000e-01, -1.3388500e-01,\n",
       "       -7.6028000e-01, -6.5597800e-01,  5.3950000e-02,  4.7608000e-02,\n",
       "       -4.5350000e-02, -1.2116990e+00, -4.6978110e-01,  7.8391600e-01,\n",
       "        5.5862000e-01,  2.1837850e-01, -1.2795440e+00, -3.2828900e-01,\n",
       "        3.3790000e-03,  1.2092100e-01,  1.5947100e+00,  5.3854200e-01,\n",
       "       -2.9418900e+00, -7.3483600e-01,  4.0644000e-01,  7.7790000e-01,\n",
       "        5.8193700e-01,  1.0549600e+00,  6.3846700e-01, -1.6737800e+00,\n",
       "       -9.2511400e-01,  5.8266300e-01,  2.4741600e+00, -5.2465500e-02,\n",
       "        3.0074000e-02, -6.1616700e-01, -1.5598500e-01, -1.2296787e-01,\n",
       "       -3.1101800e-01, -9.7380000e-02, -2.5993100e-01,  1.0612600e-01,\n",
       "       -2.3078000e-01,  6.0190900e-01, -9.8088700e-01, -4.7613160e-01,\n",
       "        1.7464400e-01,  3.4873200e-01,  5.6544000e-02, -5.0230300e-02,\n",
       "        8.3249900e-01,  2.4496000e-01,  4.1968000e-01,  6.8782800e-01,\n",
       "       -9.9132000e+00, -3.7726200e-01,  1.0895050e+00, -3.6040000e-01,\n",
       "       -1.2254210e+00,  5.1722400e-01,  4.4490770e-01,  3.1119800e-01,\n",
       "       -6.5723100e-01,  1.1435000e-01,  2.7204900e-01, -9.2979600e-02,\n",
       "       -6.3254000e-01, -7.9852300e-01,  4.7183000e-01, -1.5235300e+00,\n",
       "        4.5486000e-01,  2.5913900e-01,  4.2466820e-01, -6.2467000e-01,\n",
       "        8.1188100e-02, -1.7827110e+00, -4.5339500e-01,  4.2380000e-02])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"Hey! how have you been?\"\n",
    "doc_embedding = get_document_embedding(doc, gloveModel)\n",
    "doc_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying for All Docs\n",
    "\n",
    " - Store All Document Vectors into a Dictionary at respective indices\n",
    " - Create an array having all document vectors stacked to make a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs, gloveModel):\n",
    "    \n",
    "    index_to_DocVector = {}\n",
    "    document_vectors = []\n",
    "    \n",
    "    for index, doc in enumerate(all_docs):\n",
    "        \n",
    "        \n",
    "        document_embedding = get_document_embedding(doc, gloveModel)\n",
    "        \n",
    "        document_vectors.append(document_embedding)\n",
    "        \n",
    "        index_to_DocVector[index] = document_embedding\n",
    "    \n",
    "    \n",
    "    document_embedding_matrix = np.vstack(document_vectors)\n",
    "    \n",
    "    return document_embedding_matrix, index_to_DocVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shekhartanwar/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "all_document_embeddings, index_to_DocVector = get_document_vecs(load_data(), gloveModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 34660\n",
      "shape of document_vecs (34660, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of dictionary {len(index_to_DocVector)}\")\n",
    "print(f\"shape of document_vecs {all_document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"I really liked the product. My kids use it all the time!\"\n",
    "\n",
    "doc_embedding = get_document_embedding(doc, gloveModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21231"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax((cosine_similarity(all_document_embeddings, doc_embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21231"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we look for a review that is similar to our review\n",
    "data = load_data()\n",
    "similar_document_index = np.argmax(cosine_similarity(all_document_embeddings, doc_embedding))\n",
    "similar_document_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a review of the Kindle Paperwhite launched July 2015. Essentially. the same as the previous Kindle Paperwhite but with a fantastic upgraded (300 dpi) screen, more memory and storage it's a terrific reading device. This review aims to describe both this product, and compare it with the other Kindle devices available to help you choose the best one for you.Before I start, Ive noticed several complains about Kindle not supporting the ePub format used by Canadian public libraries. Dont be put off, theres a solution in the Frequently Asked Questions section below.This review is broken up into sections so you can skip the less interesting bits. If youre in a hurry Ive included a summary at the start, and a Frequently Asked Questions section at the end.Summary - What can you do with it-----------------------------------------* Read a book. It can store 1,000s of books - take an entire library away with you on holiday.* Buy a book. You can browse the Amazon kindle store, and buy books over a WIFI connection* Borrow a book. Using the Kindle Owners Library you can borrow one of 60,000 books (if you have Amazon Prime Membership)* Share your books. Using Amazon Family Library to link two Amazon accounts and four children and share your books with the entire family* Highlight a section. Using the touch screen, you can highlight a particulary interesting few lines. If several people highlight the same section, that's also indicated.* Record a bookmark. To come back to it later* Read a book on your Kindle, continue on your smartphone or tablet computer, and it will jump to your last known position* Look up a word, and record them in the vocabulary builder* Track a characters appearance throughout a book using the X Ray feature.* Read for weeks without needing to recharge - typically 4-6 weeks* Share you books (Multiple Kindles on the same account, or family sharing)* Access every book you've ever owned on the cloud - available for download within seconds over WIFI or 3G..What Devices are available--------------------------------As of July 2015, Amazon have the following Kindle devices on sale:-1. Kindle 6 (around CDN 79)2. Kindle Paperwhite 6th Generation (212 dpi - launched October 2013) - around CDN 1292. New Kindle Paperwhite 7th Gen (300 dpi - launched July 2015) - around CDN 139-2093. Kindle Voyger - around US 219-289 (Doesnt seem to be on sale in Amazon Canada)4. Kindle Fire - 6 or 7 colour tablet computers at around CDN 300-400So you're faced with a few devices, plus additional options. Which to buy Here's a quick summary in order of cost.*** The entry level Kindle ***An entry level black and white (e-ink) book reader, it has a 6 inch touch screen with enough storage to hold thousands of books, and a battery which should last around 4 weeks.A terrific device to read books, and a worthy contender. For the technically minded, it has a 1Ghz processor and 4Gb of memory with an E-ink display. Essentially this means it's (a) Black and White (b) Fantastic to read even in bright sunlight (c) Requires very little power - hence the battery lasts for weeks instead of hours.*** New Kindle Paperwhite ***Launched in 2015, this is really an upgrade adding a super-sharp display with 300 pixels per inch - the same as the top of the range Voyage. The high resolution screen does provide a crisp text which is pleasing to the eye, but battery life is reduced from 8 weeks of the original Paperwhite to 6 weeks - but still a very long time.Compared to the existing (2013) Paperwhite it also includes double the memory which means its faster and slicker than before, and a new Bookerly font plus better presentation of fonts on screen.If you have an existing Paperwhite its probably not worth upgrading. However, if you own a prior device, youll notice the huge leap in screen quality, contrast and performance.*** The Voyage ***Far more expensive that Paperwhite, this is thinner and 26g lighter. It has more of a quality feel with the screen flush with the e-reader body which makes it look more like a tablet, and keeps dirt from getting caught in the edges. The back has a soft touch plastic finish which feels lovely in the hand, and it has new Page Press buttons on the side to turn the pages. When the buttons are pressed theres a gentle vibration for feedback (nice touch). It has 4Gb of storage (same as the Paperwhite), and a similar battery which should last around 6 weeks on a single charge.Although it has the same sharp (300 dpi) screen as the Paperwhite, the backlight includes an ambient light sensor so it automatically adjusts to different lighting conditions. This means if you read it the dark it will automatically adjust the brightness so it doesnt look too bright. In addition, youll find when you switch off the room light your eyes naturally become accustomed to the darkness. To compensate, the Voyage automatically softens the light slightly. A lovely example of attention to detail in this premium device.*** Kindle Fire Tablet Computer ***The Kindle Fire range is a very different beast to the rest of the Kindle range. Similar to the Apple iPad mini, it has a high resolution screen, a fast processor and lots of memory with a colour screen and a much shorter battery life - hours rather than weeks. Although you can read books on a Fire using the Amazon Kindle App, youll find the Kindle with its sharp black and white screen far more like reading a physical book. Youll find it produces less eye strain, and whereas a tablet computer is better suited to surfing the web, watching videos and reading eMails, the Kindle is a dedicated book reader device.Unlike a Kindle, the Fire produces light at the blue end of the spectrum which means it may make it harder for you to sleep. The Paperwhite and Voyage also ave the advantage of a soft white back-light which is far more suited to reading in bed, and it wont for example, wake your partner if reading in the dark.Finally, unlike the rest of the Kindle range, the Fire is almost useless in bright sunlight so while its fine for everyday use, its less useful on holiday. The Kindle range of devices are perfect to take away on holiday as they can carry thousands of books, and are a pleasure to read even in bright sunlight.What other options do I have----------------------------------Having decided upon the model (Kindle, Paperwhite, Voyage or Fire), the next decision is around:-* Special Offers - ie. adverts* 3G or WIFI connectionPersonally I have no issue with adverts on a Kindle. They only appear on screen when the screen saver is on, and are not at all distracting.The 3G option is a more difficult question. This effectively means you can use your kindle to buy or download books over-the-air instead of at home or near a WIFI hotspot. Personally I went for the WIFI model as the saving is considerable, but its a personal choice. Of course if you wont have WIFI at home, then youll need the 3G option.One point worth mentioning however, is if you have a modern smartphone, you can always create a Personal Hotspot or local WIFI signal that your Kindle (or indeed any other WIFI device) can connect to. Got to Settings Personal Hotspot on your mobile to set one up, and you can connect your WIFI only Kindle while youre out and about.Conclusion-------------If you love books but your on a budget, the original Kindle with WIFI and special offers is a great option. Youll still be able to carry thousands of book around with you, but youll lose out on the soft white back-light and super crisp display of the Papwerwhite.The Kindle Paperwhite is your next best option and is a terrific device for reading with a superb 300 dpi super-sharp screen. Finally, theres the top of the range Voyage with the page press buttons and ambient light sensor which automatically adjusts the brightness.If youre in the market for a Tablet Computer then the Kindle Fire range is worth considering, but youll get a better book reading experience on the Kindle. Myself I have both a Paperwhite and an iPad Air 2. I was skeptical at first, but the Kindle is a perfect device for reading with one hand (on the train), and the back-light and long battery life a huge bonus.Frequently Asked Questions---------------------------------Q: What file format will a Kindle readA: Kindle Format 8 (AZW3), Kindle (AZW), TXT, PDF, unprotected MOBI, PRC natively in addition to HTML, DOC, DOCX, JPEG, GIF, PNG, BMP through conversionQ: What It doesnt support EPUB formatA: No - but you can convert your EPUB books using a freely available program called Calibre. Simple search for You Tube um0teoAR6zw and youll see a video explaining how to do this. Simples.Q: If I lose my Kindle, do I lose my books as wellA: No. All books are available on your Amazon account to download on a new device.Q: Can I save my place in the bookA: Yes. It will automatically save your place, and if you resume on other device it will prompt you to jump to that position.Q: I often need reading glasses - can I adjust the font sizeA: Yes. Theres a range of fonts, and sizes to suit all tastesQ: Can I share books with my partner or childrenA: Yes. You can now share your books using Amazon Family Library to link two Amazon accounts and four children and share your books with the entire familyQ. Can I use a Kindle to read in bedA. Yes. But only on the Paperwhite and Voyage models. Its a pleasure to read in the dark with a soft white light. For the base Kindle model youll need the light onQ. Are there any free books availableA. Yes! Over 40,000 classic titles are available for free (out of copyright), and they can be legally downloaded. If you have a Amazon Prime membership you can also borrow a book from the Kindle Owners Library which includes around 60,000 books.Q: How long does the battery lastA: Depending upon model, you should get between 4 and 6 weeks between charges on around 30 minutes of reading per day. Ive certainly found Ive gone for weeks without charging the deviceQ: How long does it take to chargeA: From absolutely flat, around 4 hoursQ: How many books will the Kindle holdA: Literally thousands. You can also download every book youve ever owned from your Amazon account - so the list is quite literally unlimited.Q: Do I need anything elseA: Youll need a smartphone charger or laptop with a USB socket as the Kindle range doesnt come with a charger (but the Fire does). Amazon also offer an additional charger to buy if you need it. Some people also buy a cover, but I think this is not really needed. It adds to the weight, and is not really needed. It will protect your Kindle from a drop however, and protect the screen if its left knocking around in a bag. Superb reading device - but which one's best for you\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[similar_document_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation : \n",
    "\n",
    "Even though the returned text is too long, however if you copy the text on a text pad say sublime or any other software, you will find all the words in the original document in the returned document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing\n",
    "\n",
    "We will improve on the lookup feature developed. Instead of scanning through the 34660 reviews, we will look at a subset of reviews, this will accompalish two tasks : \n",
    "\n",
    "1) Returned Reviews will be most similar ( based on LSH )\n",
    "\n",
    "\n",
    "2) Since only a subset of reviews will be focussed on, this will speed up the retrieval time and will extract only the nearest neighbour to the review we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 34660 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(data)       # This many vectors.\n",
    "N_DIMS = len(index_to_DocVector[1])     # Vector dimensionality.\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the number of planes\n",
    "\n",
    "* Each plane divides the space to $2$ parts.\n",
    "* So $n$ planes divide the space into $2^{n}$ hash buckets.\n",
    "* We want to organize 34,600 document vectors into buckets so that every bucket has about $~16$ vectors.\n",
    "* For that we need $\\frac{34600}{16}=2163$ buckets.\n",
    "* We're interested in $n$, number of planes, so that $2^{n}= 2163$. Now, we can calculate $n=\\log_{2}2163 = 11.07 \\approx 11$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of planes. We use log2(625) to have ~16 vectors/bucket.\n",
    "N_PLANES = 11\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the hash number for a vector\n",
    "\n",
    "For each vector, we need to get a unique number associated to that vector in order to assign it to a \"hash bucket\".\n",
    "\n",
    "### Hyperlanes in vector spaces\n",
    "* In $3$-dimensional vector space, the hyperplane is a regular plane. In $2$ dimensional vector space, the hyperplane is a line.\n",
    "* Generally, the hyperplane is subspace which has dimension $1$ lower than the original vector space has.\n",
    "* A hyperplane is uniquely defined by its normal vector.\n",
    "* Normal vector $n$ of the plane $\\pi$ is the vector to which all vectors in the plane $\\pi$ are orthogonal (perpendicular in $3$ dimensional case).\n",
    "\n",
    "### Using Hyperplanes to split the vector space\n",
    "We can use a hyperplane to split the vector space into $2$ parts.\n",
    "* All vectors whose dot product with a plane's normal vector is positive are on one side of the plane.\n",
    "* All vectors whose dot product with the plane's normal vector is negative are on the other side of the plane.\n",
    "\n",
    "### Encoding hash buckets\n",
    "* For a vector, we can take its dot product with all the planes, then encode this information to assign the vector to a single hash bucket.\n",
    "* When the vector is pointing to the opposite side of the hyperplane than normal, encode it by 0.\n",
    "* Otherwise, if the vector is on the same side as the normal vector, encode it by 1.\n",
    "* If you calculate the dot product with each plane in the same order for every vector, you've encoded each vector's unique hash ID as a binary number, like [0, 1, 1, ... 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hash table `hashes` is list of `N_UNIVERSES` matrices, each describes its own hash table. Each matrix has `N_DIMS` rows and `N_PLANES` columns. Every column of that matrix is a `N_DIMS`-dimensional normal vector for each of `N_PLANES` hyperplanes which are used for creating buckets of the particular hash table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will complete the function `hash_value_of_vector` which places vector `v` in the correct hash bucket.\n",
    "\n",
    "* We will First multiply the vector `v`, with a corresponding plane. This will give us a vector of dimension $(1,\\text{N_planes})$.\n",
    "* We will then convert every element in that vector to 0 or 1.\n",
    "* We create a hash vector by doing the following: if the element is negative, it becomes a 0, otherwise you change it to a 1.\n",
    "* We then compute the unique number for the vector by iterating over `N_PLANES`\n",
    "* Then We multiply $2^i$ times the corresponding bit (0 or 1).\n",
    "* We will then store that sum in the variable `hash_value`.\n",
    "\n",
    "**Intructions:** Create a hash for the vector in the function below.\n",
    "Use this formula:\n",
    "\n",
    "$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the sets of planes\n",
    "* Create multiple (25) sets of planes (the planes that divide up the region).\n",
    "* You can think of these as 25 separate ways of dividing up the vector space with a different set of planes.\n",
    "* Each element of this list contains a matrix with 300 rows (the word vector have 300 dimensions), and 11 columns (there are 11 planes in each \"universe\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planes():\n",
    "    np.random.seed(0)\n",
    "    planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]\n",
    "    return planes_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    \n",
    "    dot_product = np.dot(v,planes)\n",
    "    \n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "    \n",
    "    h = sign_of_dot_product > 0\n",
    "    \n",
    "    h = np.squeeze(h)\n",
    "    \n",
    "    hash_value = 0\n",
    "    \n",
    "    number_planes = planes.shape[1]\n",
    "    \n",
    "    for i in range(number_planes):\n",
    "        hash_value = hash_value + np.power(2,i) * h[i]\n",
    "    \n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The hash value for this vector, and the set of planes at index 0, is 512\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "idx = 0\n",
    "planes_l = create_planes()\n",
    "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "print(f\" The hash value for this vector,\",\n",
    "      f\"and the set of planes at index {idx},\",\n",
    "      f\"is {hash_value_of_vector(vec, planes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](LSH.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(document_vecs, planes):\n",
    "    \n",
    "    number_of_planes = planes.shape[1]\n",
    "    \n",
    "    number_buckets = 2**number_of_planes\n",
    "    \n",
    "    hash_table = {i:[] for i in range(number_buckets)}\n",
    "    \n",
    "    index_table = {i:[] for i in range(number_buckets)}\n",
    "    \n",
    "    for i,v in enumerate(document_vecs):\n",
    "        \n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "        \n",
    "        \n",
    "        hash_table[hash_value].append(v)\n",
    "        \n",
    "        index_table[hash_value].append(i)\n",
    "        \n",
    "        \n",
    "    return hash_table, index_table\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing For One Plane in 0th Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash table at key 0 has 162 document vectors\n",
      "The id table at key 0 has 162\n",
      "The first 5 document indices stored at key 0 of are [309, 528, 668, 783, 1007]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "planes = planes_l[0]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(all_document_embeddings, planes)\n",
    "\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
    "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Planes in All Universes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hash_index_tables(N_UNIVERSES):\n",
    "    \n",
    "\n",
    "    hash_tables, id_tables = [],[]\n",
    "\n",
    "    for i in range(N_UNIVERSES):\n",
    "        print('working on hash universe #:', i)\n",
    "        planes = planes_l[i]\n",
    "        hash_table, id_table = make_hash_table(all_document_embeddings, planes)\n",
    "\n",
    "        hash_tables.append(hash_table)\n",
    "\n",
    "        id_tables.append(id_table)\n",
    "\n",
    "    return hash_tables,id_tables\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "hash_tables,id_tables = get_all_hash_index_tables(N_UNIVERSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(vector, document_vectors, K):\n",
    "    \n",
    "    \n",
    "    # vector here is the vector we are interested in\n",
    "    \n",
    "    similarity_l = []\n",
    "    for v in document_vectors:\n",
    "        \n",
    "        cosine_similarity_value = cosine_similarity(vector, v)\n",
    "        similarity_l.append(cosine_similarity_value)\n",
    "        \n",
    "    sorted_indexes = np.argsort(similarity_l)\n",
    "    \n",
    "    k_th_index = sorted_indexes[-K:]\n",
    "    \n",
    "    return k_th_index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_KNN(search_vector_document_id , search_vector, K, planes_l, number_universes, hash_tables,id_tables):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT :- \n",
    "        \n",
    "    search_vector_document_id :  The vector which we are looking for, this is the document id for that\n",
    "    search_vector             :  The vector's embedding\n",
    "    K                         :  The number of neighbours we are interested in (user defined)\n",
    "    planes_l                  :  All the planes which correspond to a particular universe\n",
    "    number_universes          :  All the universes which we are using (here 25)\n",
    "    hash_tables               :  These are all the tables present in the universes, for each sepcific universe there \n",
    "                                 would be a specific hash table\n",
    "    id_tables                 :  These are all the tables which contains the id number of the document present in the\n",
    "                                 dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    vectors_to_consider = []\n",
    "    \n",
    "    indexes_to_consider = []\n",
    "    \n",
    "    indexes_to_consider_set = set()\n",
    "    \n",
    "    \n",
    "    for universe_id in range(number_universes):\n",
    "        \n",
    "        # get all the planes in that universe\n",
    "        planes = planes_l[universe_id]\n",
    "        \n",
    "        \n",
    "        # in those planes, get the hash value of the search_vector\n",
    "        hash_value = hash_value_of_vector(search_vector, planes)\n",
    "        \n",
    "        # get all the hash tables in that universe\n",
    "        hash_table = hash_tables[universe_id]\n",
    "        \n",
    "        # In those hash tables look for all the document vectors which had the hash_value calculated above\n",
    "        document_vectors_at_hash_value = hash_table[hash_value]\n",
    "        \n",
    "        \n",
    "        # get all index tables in that universe\n",
    "        index_table = id_tables[universe_id]\n",
    "        \n",
    "        # get indexes of all the documents in that index table\n",
    "        document_indexes_to_consider = index_table[hash_value]\n",
    "        \n",
    "        if search_vector_document_id in document_indexes_to_consider:\n",
    "            document_indexes_to_consider.remove(search_vector_document_id)\n",
    "            print(f\"removed doc_id {search_vector_document_id} of input vector from document_indexes_to_consider\")\n",
    "        \n",
    "        for i,id_number in enumerate(document_indexes_to_consider):\n",
    "            \n",
    "            \n",
    "            if id_number not in indexes_to_consider_set:\n",
    "                \n",
    "                document_vector_at_id_number = document_vectors_at_hash_value[i]\n",
    "                \n",
    "                vectors_to_consider.append(document_vector_at_id_number)\n",
    "                \n",
    "                indexes_to_consider.append(id_number)\n",
    "                \n",
    "                \n",
    "                indexes_to_consider_set.add(id_number)\n",
    "        \n",
    "        # Now run k-NN on the smaller set of vecs-to-consider.\n",
    "        print(\"Fast considering %d vecs\" % len(vectors_to_consider))\n",
    "        \n",
    "        # convert the vecs to consider set to a list, then to a numpy array\n",
    "        vecs_to_consider_arr = np.array(vectors_to_consider)\n",
    "        \n",
    "        # call nearest neighbors on the reduced list of candidate vectors\n",
    "        nearest_neighbor_idx_l = nearest_neighbor(search_vector, vecs_to_consider_arr, K)\n",
    "        \n",
    "        \n",
    "        # Use the nearest neighbor index list as indices into the ids to consider\n",
    "        # create a list of nearest neighbors by the document ids\n",
    "        nearest_neighbor_ids = [indexes_to_consider[idx]\n",
    "                                for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "        return nearest_neighbor_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing On 1 document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "\n",
    "doc_to_search = data[doc_id]\n",
    "vec_to_search = all_document_embeddings[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast considering 8643 vecs\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "nearest_neighbor_ids  = approximate_KNN(doc_id , vec_to_search, K, planes_l, N_UNIVERSES, hash_tables,id_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: This product so far has not disappointed. My children love to use it and I like the ability to monitor control what content they see with ease. Kindle\n",
      "\n",
      "Nearest neighbor at document id 15478\n",
      "document contents: This item works great and fitst anywhere. I have no complaints... Good buy\n",
      "Nearest neighbor at document id 29732\n",
      "document contents: I have the stick also much better than the stick one... Great buy\n",
      "Nearest neighbor at document id 3\n",
      "document contents: I've had my Fire HD 8 two weeks now and I love it. This tablet is a great value.We are Prime Members and that is where this tablet SHINES. I love being able to easily access all of the Prime content as well as movies you can download and watch laterThis has a 1280/800 screen which has some really nice look to it its nice and crisp and very bright infact it is brighter then the ipad pro costing $900 base model. The build on this fire is INSANELY AWESOME running at only 7.7mm thick and the smooth glossy feel on the back it is really amazing to hold its like the futuristic tab in ur hands. Good!!!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {data[neighbor_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Congratulations - Now you can look up vectors that are similar to the encoding of your document using LSH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "furniture_search",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "78390c404f2e41ae9459de178d904214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_63426df5e8084e30a1f98d1513e3b873",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e10e4903e0074248a95e6fd6b8add412",
              "IPY_MODEL_d3b2e28f2b8b46c785bcfd12d13df877",
              "IPY_MODEL_45f1616e7a1a4d169d6220cfb4c05cd2"
            ]
          }
        },
        "63426df5e8084e30a1f98d1513e3b873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e10e4903e0074248a95e6fd6b8add412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d9570797f04b45178ab0be6b6c130355",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8c2c383677b459886fa6c39ed38f008"
          }
        },
        "d3b2e28f2b8b46c785bcfd12d13df877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_55b3cca22ecd42be9d0d664b7495fd78",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1012,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30588c74a9a040d597a00139ee8a3076"
          }
        },
        "45f1616e7a1a4d169d6220cfb4c05cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3253b960bf93428abac5c8d9b5b7dbd2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1012 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e07ec52cd1e7485eb5062dc9633ae0ec"
          }
        },
        "d9570797f04b45178ab0be6b6c130355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8c2c383677b459886fa6c39ed38f008": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55b3cca22ecd42be9d0d664b7495fd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30588c74a9a040d597a00139ee8a3076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3253b960bf93428abac5c8d9b5b7dbd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e07ec52cd1e7485eb5062dc9633ae0ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "oEUAGzGKnArk",
        "outputId": "eb464448-d5d5-4e0b-e5c7-43ffdaa4b6b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 52.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 49.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install timm\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm.autonotebook import tqdm\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XD94-JOnQgz",
        "outputId": "0692bd57-1c1b-496d-dd22-b0994dc03065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsEfzG-CnTbY",
        "outputId": "ea7a0d77-d3a2-4b82-cdea-0b35b130832a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/gdrive/My Drive/Colab Notebooks/nth-honor-339920-5256e664e9e6.json\""
      ],
      "metadata": {
        "id": "I9KC7yaVnipK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#image_2_text_data=pd.read_csv(\"gdrive/MyDrive/Furniture-Dataset/all_descriptions.txt\",header=None)"
      ],
      "metadata": {
        "id": "ysMLC2t2oNC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image_2_text_data[0][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "SVzQcBUtuzwB",
        "outputId": "f96ae39f-5dc5-46ea-dc64-4392df4b5797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Panache Two Seater Sofa in Half Leather In Sky Blue Colour by Furnitech._image_0.jpg#2\\tsky blue color two seater sofa with quilted pattern backrest'"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#updated_descriptions=pd.DataFrame(image_2_text_data[0].str.split('\\t').to_list(),columns=['image','caption'])"
      ],
      "metadata": {
        "id": "d6kuV9F3ojsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#updated_descriptions.to_csv(\"image_2_captions.csv\")"
      ],
      "metadata": {
        "id": "0w6nFuFxym5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#captions=pd.read_csv(\"gdrive/MyDrive/Furniture-Dataset/image_2_captions.csv\")"
      ],
      "metadata": {
        "id": "tKTC6JemnwPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#captions.caption.isna().value_counts()"
      ],
      "metadata": {
        "id": "kxzAwwhu10hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#captions['id']=[id_ for id_ in range(captions.shape[0] // 3) for _ in range(3)]"
      ],
      "metadata": {
        "id": "FwME7x4b12Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#captions.drop(columns='Unnamed: 0',inplace=True)"
      ],
      "metadata": {
        "id": "ePrTH8Na2PFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#captions.to_csv(\"captions.csv\",index=False)"
      ],
      "metadata": {
        "id": "KUnascXj2QuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##start from here\n",
        "df = pd.read_csv(\"gdrive/MyDrive/Furniture-Dataset/captions.csv\")\n",
        "image_path = \"gdrive/MyDrive/Furniture-Dataset/PepperFry_SampleImages\"\n",
        "captions_path = \"gdrive/MyDrive/Furniture-Dataset/\""
      ],
      "metadata": {
        "id": "8pSe1_Fw270q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Myx63UUCGGPm",
        "outputId": "b2fb4b8c-d457-4b31-d3d7-277366591e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-962beea5-dd95-4d35-928d-93590dc6a104\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A child in a pink dress is climbing up a set o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A girl going into a wooden building .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing into a wooden playhouse .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing the stairs to her playh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl in a pink dress going into a woo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-962beea5-dd95-4d35-928d-93590dc6a104')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-962beea5-dd95-4d35-928d-93590dc6a104 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-962beea5-dd95-4d35-928d-93590dc6a104');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                       image  ... id\n",
              "0  1000268201_693b08cb0e.jpg  ...  0\n",
              "1  1000268201_693b08cb0e.jpg  ...  0\n",
              "2  1000268201_693b08cb0e.jpg  ...  0\n",
              "3  1000268201_693b08cb0e.jpg  ...  0\n",
              "4  1000268201_693b08cb0e.jpg  ...  0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CFG"
      ],
      "metadata": {
        "id": "z_TuN9Ca3wt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    debug = False\n",
        "    image_path = image_path\n",
        "    captions_path = captions_path\n",
        "    batch_size = 32\n",
        "    num_workers = 2\n",
        "    head_lr = 1e-3\n",
        "    image_encoder_lr = 1e-4\n",
        "    text_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-3\n",
        "    patience = 1\n",
        "    factor = 0.8\n",
        "    epochs = 4\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_name = 'resnet50'\n",
        "    image_embedding = 2048\n",
        "    text_encoder_model = \"distilbert-base-uncased\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"distilbert-base-uncased\"\n",
        "    max_length = 200\n",
        "\n",
        "    pretrained = True # for both image encoder and text encoder\n",
        "    trainable = True # for both image encoder and text encoder\n",
        "    temperature = 1.0\n",
        "\n",
        "    # image size\n",
        "    size = 226\n",
        "\n",
        "    # for projection head; used for both image and text encoders\n",
        "    num_projection_layers = 1\n",
        "    projection_dim = 256 \n",
        "    dropout = 0.1"
      ],
      "metadata": {
        "id": "Hx2E7PJ33mI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "OGCzvdhN4OYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter:\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.avg, self.sum, self.count = [0] * 3\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        text = f\"{self.name}: {self.avg:.4f}\"\n",
        "        return text\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group[\"lr\"]"
      ],
      "metadata": {
        "id": "LD4o1qlX3yiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the tittle image of this article, we need to encode both images and their describing texts. So, the dataset needs to return both images and texts. Of course we are not going to feed raw text to our text encoder! We will use DistilBERT model (which is smaller than BERT but performs nearly as well as BERT) from HuggingFace library as our text encoder; so, we need to tokenize the sentences (captions) with DistilBERT tokenizer and then feed the token ids (input_ids) and the attention masks to DistilBERT. Therefore, the dataset needs to take care of the tokenization as well. Below you can see the dataset's code. Below that I'll explain the most important things that is happening in the code.\n",
        "\n",
        "In the __init__ we receive a tokenizer object which is actually a HuggingFace tokinzer; this tokenizer will be loaded when running the model. We are padding and truncating the captions to a specified max_length. In the __getitem__ we will first load an encoded caption which is a dictionary with keys input_ids and attention_mask, make tensors out of its values and after that we will load the corresponding image, transform and augment it (if there is any!) and then we make it a tensor and put it in the dictionary with \"image\" as the key. Finally we put the raw text of the caption with the key \"caption\" in the dictionary only for visualization purposes.\n",
        "\n",
        "I did not use additional data augmentations but you can add them if you want to improve the model's performance.\n"
      ],
      "metadata": {
        "id": "c44509wf4anH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        \"\"\"\n",
        "        image_filenames and cpations must have the same length; so, if there are\n",
        "        multiple captions for each image, the image_filenames must have repetitive\n",
        "        file names \n",
        "        \"\"\"\n",
        "\n",
        "        self.image_filenames = image_filenames\n",
        "        print(self.image_filenames)\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: torch.tensor(values[idx])\n",
        "            for key, values in self.encoded_captions.items()\n",
        "        }\n",
        "        try:\n",
        "          image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
        "          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "          image = self.transforms(image=image)['image']\n",
        "          item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "          item['caption'] = self.captions[idx]\n",
        "        #print(item)\n",
        "          return item\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"exception in processing\")\n",
        "          print(self.image_filenames[idx])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "\n",
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\":\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )"
      ],
      "metadata": {
        "id": "BAA51Dra4R8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Encoder\n",
        "\n",
        "The image encoder code is straight forward. I'm using PyTorch Image Models library (timm) here which makes a lot of different image models available from ResNets to EfficientNets and many more. Here we will use a ResNet50 as our image encoder. You can easily use torchvision library to use ResNets if you don't want to install a new library.\n",
        "\n",
        "The code encodes each image to a fixed size vector with the size of the model's output channels (in case of ResNet50 the vector size will be 2048). This is the output after the nn.AdaptiveAvgPool2d() layer.\n"
      ],
      "metadata": {
        "id": "JvdJRxcO4nmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encode images to a fixed size vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "gnYzDUVi4hsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Encoder\n",
        "\n",
        "As I mentioned before, I'll use DistilBERT as the text encoder. Like its bigger brother BERT, two special tokens will be added to the actual input tokens: CLS and SEP which mark the start and end of a sentence. To grab the whole representation of a sentence (as the related BERT and DistilBERT papers point out) we use the final representations of the CLS token and we hope that this representation captures the overall meaning of the sentence (caption). Thinking it in this way, it is similar to what we did to images and converted them into a fixed size vector.\n",
        "\n",
        "In the case of DistilBERT (and also BERT) the output hidden representation for each token is a vector with size 768. So, the whole caption will be encoded in the CLS token representation whose size is 768.\n"
      ],
      "metadata": {
        "id": "W2V-mRkP4x9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = DistilBertModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = DistilBertModel(config=DistilBertConfig())\n",
        "            \n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "        # we are using the CLS token hidden representation as the sentence's embedding\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]"
      ],
      "metadata": {
        "id": "rvNlGaQZ4z48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projection Head\n",
        "\n",
        "I used Keras code example implementation of projection head to write the following in PyTorch. Now that we have encoded both our images and texts into fixed size vectors (2048 for image and 768 for text) we need to bring (project) them into a new world (!) with similar dimensions for both images and texts in order to be able to compare them and push apart the non-relevant image and texts and pull together those that match. So, the following code will bring the 2048 and 768 dimensional vectors into a 256 (projection_dim) dimensional world, where we can compare them.\n",
        "\n",
        "\"embedding_dim\" is the size of the input vector (2048 for images and 768 for texts) and \"projection_dim\" is the the size of the output vector which will be 256 for our case. For understanding the details of this part you can refer to the CLIP paper.\n"
      ],
      "metadata": {
        "id": "Pr44bwFU49iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        projection_dim=CFG.projection_dim,\n",
        "        dropout=CFG.dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jVl98omg44hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP\n",
        "\n",
        "This part is where all the fun happens! I'll also talk about the loss function here. I translated some of the code from Keras code examples into PyTorch for writing this part. Take a look at the code and then read the explanation below this code block.\n",
        "\n",
        "Here we will use the previous modules that we built to implement the main model. The __init__ function is self-explanatory. In the forward function, we first encode the images and texts separately into fixed size vectors (with different dimensionalities). After that, using separate projection modules we project them to that shared world (space) that I talked about previously. Here the encodings will become of similar shape (256 in our case). After that we will compute the loss. Again I recommend reading CLIP paper to get it better but I'll try my best to explain this part.\n",
        "\n",
        "In Linear Algebra, one common way to measure if two vectors are of similar characteristics (they are like each other) is to calculate their dot product (multiplying the matching entries and take the sum of them); if the final number is big, they are alike and if it is small they are not (relatively speaking)!\n"
      ],
      "metadata": {
        "id": "mxoSUbQj5JbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay! What I just said is the most important thing to have in mind to understand this loss function. Let's continue. We talked about two vectors, but, what do we have here? We have image_embeddings, a matrix with shape (batch_size, 256) and text_embeddings with shape (batch_size, 256). Easy enough! it means we have two groups of vectors instead of two single vectors. How do we measure how similar two groups of vectors (two matrices) are to each other? Again, with dot product (@ operator in PyTorch does the dot product or matrix multiplication in this case). To be able to multiply these two matrices together, we transpose the second one. Okay, we get a matrix with shape (batch_size, batch_size) which we will call logits. (temperature is equal to 1.0 in our case, so, it does not make a difference. You can play with it and see what difference it makes. Also look at the paper to see why it is here!).\n",
        "\n",
        "I hope you are still with me! If not it's okay, just review the code and check their shapes. Now that we have our logits, we need targets. I need to say that there is a more straight forward way to obtain targets but I had to do this for our case (I'll talk about why in a next paragraph).\n",
        "\n",
        "Let's consider what we hope that this model learns: we want it to learn \"similar representations (vectors)\" for a given image and the caption describing it. Meaning that either we give it an image or the text describing it, we want it to produce same 256 sized vectors for both.\n"
      ],
      "metadata": {
        "id": "GOc5mAAY5OT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Getting Image and Text Features\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        # Getting Image and Text Embeddings (with same dimension)\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        # Calculating the Loss\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax(\n",
        "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "        )\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "AWEPH1d45B74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So logits, in the best case, will be a matrix that if we take its softmax, will have 1.0s in the diagonal (An identity matrix to call it with fancy words!). As the loss function's job is to make model's predictions similar to targets (at least in most cases!), we want such a matrix as our target. That's the reason why we are calculating images_similarity and texts_similarity matrices in the code block above.\n",
        "\n",
        "Now that we've got our targets matrix, we will use simple cross entropy to calculate the actual loss. I've written the full matrix form of cross entropy as a function which you can see in the bottom of the code block. Okay! We are done! Wasn't it simple?! Alright, you can ignore the next paragraph but if you are curious, there is an important note in that.\n",
        "\n",
        "Here's why I didn't use a simpler approach: I need to admit that there's a simpler way to calculate this loss in PyTorch; by doing this: nn.CrossEntropyLoss()(logits, torch.arange(batch_size)). Why I did not use it here? For 2 reasons. 1- The dataset we are using has multiple captions for a single image; so, there is the possibility that two identical images with their similar captions exist in a batch (it is rare but it can happen). Taking the loss with this easier method will ignore this possibility and the model learns to pull apart two representations (assume them different) that are actually the same. Obviously, we don't want this to happen so I calculated the whole target matrix in a way that takes care of these edge cases. 2- Doing it the way I did, gave me a better understanding of what is happening in this loss function; so, I thought it would give you a better intuition as well!\n"
      ],
      "metadata": {
        "id": "Jms0vEv65XKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "Here are some funtions to help us load train and valid dataloaders, our model and then train and evaluate our model on those. There's not much going on here; just simple training loop and utility functions\n"
      ],
      "metadata": {
        "id": "GG9u7guB5aqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_valid_dfs():\n",
        "    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "    image_ids = np.arange(0, max_id)\n",
        "    np.random.seed(42)\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "    return train_dataframe, valid_dataframe\n",
        "\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    transforms = get_transforms(mode=mode)\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        shuffle=True if mode == \"train\" else False,\n",
        "    )\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "wCQhFzOL5cD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    loss_meter = AvgMeter()\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
        "    for batch in tqdm_object:\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "        loss = model(batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
        "    return loss_meter\n",
        "\n",
        "\n",
        "def valid_epoch(model, valid_loader):\n",
        "    loss_meter = AvgMeter()\n",
        "\n",
        "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
        "    for batch in tqdm_object:\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "        loss = model(batch)\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
        "    return loss_meter\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_df, valid_df = make_train_valid_dfs()\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(), model.text_projection.parameters()\n",
        "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
        "    )\n",
        "    step = \"epoch\"\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"Epoch: {epoch + 1}\")\n",
        "        model.train()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_loss = valid_epoch(model, valid_loader)\n",
        "        \n",
        "        if valid_loss.avg < best_loss:\n",
        "            best_loss = valid_loss.avg\n",
        "            torch.save(model.state_dict(), \"best.pt\")\n",
        "            print(\"Saved Best Model!\")\n",
        "        \n",
        "        lr_scheduler.step(valid_loss.avg)"
      ],
      "metadata": {
        "id": "jUu7wRel5pWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "78390c404f2e41ae9459de178d904214",
            "63426df5e8084e30a1f98d1513e3b873",
            "e10e4903e0074248a95e6fd6b8add412",
            "d3b2e28f2b8b46c785bcfd12d13df877",
            "45f1616e7a1a4d169d6220cfb4c05cd2",
            "d9570797f04b45178ab0be6b6c130355",
            "c8c2c383677b459886fa6c39ed38f008",
            "55b3cca22ecd42be9d0d664b7495fd78",
            "30588c74a9a040d597a00139ee8a3076",
            "3253b960bf93428abac5c8d9b5b7dbd2",
            "e07ec52cd1e7485eb5062dc9633ae0ec"
          ]
        },
        "id": "EjWBn1TI5xP4",
        "outputId": "be1c43cd-a09e-4750-ee22-2608985508d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1000268201_693b08cb0e.jpg' '1000268201_693b08cb0e.jpg'\n",
            " '1000268201_693b08cb0e.jpg' ... '99679241_adc853a5c0.jpg'\n",
            " '99679241_adc853a5c0.jpg' '99679241_adc853a5c0.jpg']\n",
            "['1022454428_b6b660a67b.jpg' '1022454428_b6b660a67b.jpg'\n",
            " '1022454428_b6b660a67b.jpg' ... '997722733_0cb5439472.jpg'\n",
            " '997722733_0cb5439472.jpg' '997722733_0cb5439472.jpg']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78390c404f2e41ae9459de178d904214",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1012 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exception in processing\n",
            "exception in processing\n",
            "3517040752_debec03376.jpg\n",
            "3652859271_908ae0ae89.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "1689658980_0074d81d28.jpg\n",
            "3681324243_b69fa90842.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3737492755_bcfb800ed1.jpg\n",
            "3546474710_903c3c9fd3.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3694071771_ce760db4c7.jpg\n",
            "2090327868_9f99e2740d.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3325497914_f9014d615b.jpg\n",
            "2449289139_08fc1092c1.jpg\n",
            "exception in processing\n",
            "2914331767_8574e7703d.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2873648844_8efc7d78f1.jpg\n",
            "3207676216_48478bce97.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3360730513_211e1a4db6.jpg\n",
            "3085226474_62aba51179.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "469617651_278e586e46.jpg\n",
            "2657484284_daa07a3a1b.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3368207495_1e2dbd6d3f.jpg\n",
            "3183883750_b6acc40397.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2932519416_11f23b6297.jpg\n",
            "3070485870_eab1a75c6f.jpg\n",
            "exception in processing\n",
            "852469220_bc0fee3623.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2689163361_4939875be5.jpg\n",
            "3552796830_2dd2aa9c2c.jpg\n",
            "exception in processing\n",
            "1072153132_53d2bb1b60.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3626642428_3396568c3c.jpg\n",
            "2591110592_ef5f54f91c.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2651915425_7a58e862e9.jpg\n",
            "3413019648_e787f0cb88.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3356938707_d95ba97430.jpg\n",
            "1231229740_8dcbf80bfb.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "613030608_4355e007c7.jpg\n",
            "3259992638_0612a40288.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2139519215_8ca16dd192.jpg\n",
            "2261346505_302c67951d.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "294709836_87126898fb.jpg\n",
            "2189995738_352607a63b.jpg\n",
            "exception in processing\n",
            "3636796219_9916c0465a.jpg\n",
            "exception in processing\n",
            "3143159297_6f2f663ea6.jpg\n",
            "exception in processing\n",
            "1408958345_68eea9a4e4.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3148193539_de9dd48fc8.jpg\n",
            "1528205014_1323aa9dfd.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2755362721_368cbde668.jpg\n",
            "2641288004_30ce961211.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3721881082_afe9fc734e.jpg\n",
            "3381392182_db2c42430e.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "447800028_0242008fa3.jpg\n",
            "3229913073_e7857a5966.jpg\n",
            "exception in processing\n",
            "3547524138_4157f660b0.jpg\n",
            "exception in processing\n",
            "3154641421_d1b9b8c24c.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3302804312_0272091cd5.jpg\n",
            "3638688673_176f99d7fd.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3373544964_c9f1253b7d.jpg\n",
            "exception in processing\n",
            "3393343330_b13df4d8ec.jpg\n",
            "2695093520_5cfeb0729d.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "477254932_56b48d775d.jpg\n",
            "533713007_bf9f3e25b4.jpg\n",
            "exception in processing\n",
            "3455898176_f0e003ce58.jpg\n",
            "exception in processing\n",
            "2869491449_1041485a6b.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "284279868_2ca98e3dcd.jpg\n",
            "2963573792_dd51b5fbfb.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "1420060020_7a6984e2ea.jpg\n",
            "241345811_46b5f157d4.jpg\n",
            "exception in processing\n",
            "2592019072_a6c0090da4.jpg\n",
            "exception in processing\n",
            "411175971_0fffd3b8c6.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2763601657_09a52a063f.jpg\n",
            "2095478050_736c4d2d28.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "1189977786_4f5aaed773.jpg\n",
            "3358682439_be4b83544c.jpg\n",
            "exception in processing\n",
            "3402638444_dab914a3de.jpg\n",
            "exception in processing\n",
            "2613209320_edf6a2b7e9.jpg\n",
            "exception in processing\n",
            "3078229723_2aa52600de.jpg\n",
            "exception in processing\n",
            "2744330402_824240184c.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2696951725_e0ae54f6da.jpg\n",
            "2588927489_f4da2f11ec.jpg\n",
            "exception in processing\n",
            "2434006663_207a284cec.jpg\n",
            "exception in processing\n",
            "2073105823_6dacade004.jpg\n",
            "exception in processing\n",
            "3043501068_be58ac47e1.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "1287073593_f3d2a62455.jpg\n",
            "3680031186_c3c6698f9d.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3254817653_632e840423.jpg\n",
            "1333888922_26f15c18c3.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3220650628_4ed964e5b4.jpg\n",
            "2789648482_1df61f224a.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2982881046_45765ced2c.jpg\n",
            "749840385_e004bf3b7c.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2695962887_a1647c567b.jpg\n",
            "3460458114_35037d4d4c.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-b31b4f3a87ac>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b31b4f3a87ac>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, lr_scheduler, step)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mloss_meter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAvgMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtqdm_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_object\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"caption\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\", line 86, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exception in processing\n",
            "exception in processing\n",
            "3239021459_a6b71bb400.jpg\n",
            "2482629385_f370b290d1.jpg\n",
            "exception in processing\n",
            "3032790880_d216197d55.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "1573017288_4d481856e2.jpg\n",
            "2797188545_aeb26c54c0.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "1685463722_55843b6d3c.jpg\n",
            "2245914678_1f82fc3d80.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2444935470_7b0226b756.jpg\n",
            "1234817607_924893f6e1.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2827964381_408a310809.jpg\n",
            "140526326_da07305c1c.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "1803631090_05e07cc159.jpg\n",
            "2867968184_908d87cf2c.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2215136723_960edfea49.jpg\n",
            "2673148534_8daf0de833.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3221036999_3f7b152d8a.jpg\n",
            "exception in processing\n",
            "3474958471_9106beb07f.jpg\n",
            "1252787177_4b08625897.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2479652566_8f9fac8af5.jpg\n",
            "519167484_ee03e2a91e.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "143688205_630813a466.jpg\n",
            "2789238858_14261dd25a.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2481367956_8577d2fa98.jpg\n",
            "2888658480_e922a3dec2.jpg\n",
            "exception in processing\n",
            "3042488474_0d2ec81eb8.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3454149297_01454a2554.jpg\n",
            "2263655670_517890f5b7.jpg\n",
            "exception in processing\n",
            "3670918456_68631d362a.jpg\n",
            "exception in processing\n",
            "3539840291_1c3eed701d.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2055646179_169807fed4.jpg\n",
            "3677239603_95865a9073.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "3585123310_9a8e94bd2b.jpg\n",
            "exception in processing\n",
            "2344875609_8e172d682b.jpg\n",
            "496606439_9333831e73.jpg\n",
            "exception in processing\n",
            "252578659_9e404b6430.jpg\n",
            "exception in processing\n",
            "2659183350_730951f740.jpg\n",
            "exception in processing\n",
            "exception in processing\n",
            "2004674713_2883e63c67.jpg\n",
            "241346260_f50d57b517.jpg\n",
            "exception in processing\n",
            "1509786421_f03158adfc.jpg\n",
            "exception in processing\n",
            "3621741935_54d243f25f.jpg\n",
            "exception in processing\n",
            "3574627719_790325430e.jpg\n",
            "exception in processing\n",
            "3487261028_30791528ec.jpg\n",
            "exception in processing\n",
            "2594336381_a93772823b.jpg\n",
            "exception in processing\n",
            "448590900_db83c42006.jpg\n",
            "exception in processing\n",
            "3174713468_e22fa7779e.jpg\n",
            "exception in processing\n",
            "3606846822_28c40b933a.jpg\n",
            "exception in processing\n",
            "2088460083_42ee8a595a.jpg\n",
            "exception in processing\n",
            "2683963310_20dcd5e566.jpg\n",
            "exception in processing\n",
            "2856456013_335297f587.jpg\n",
            "exception in processing\n",
            "3122497129_d08f5729b8.jpg\n",
            "exception in processing\n",
            "1377668044_36398401dd.jpg\n",
            "exception in processing\n",
            "2419797375_553f867472.jpg\n",
            "exception in processing\n",
            "533979933_a95b03323b.jpg\n",
            "exception in processing\n",
            "3126752627_dc2d6674da.jpg\n",
            "exception in processing\n",
            "3259992638_0612a40288.jpg\n",
            "exception in processing\n",
            "2991575785_bd4868e215.jpg\n",
            "exception in processing\n",
            "566397227_a469e9e415.jpg\n",
            "exception in processing\n",
            "464116251_1ac4bc91f8.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "color = cv2.imread(\"gdrive/MyDrive/Furniture-Dataset/PepperFry_train/Alice Solid Wood Six Seater Dining Set in Antique Cherry Colour by home._image_0.jpg\",1 )"
      ],
      "metadata": {
        "id": "XnUth0yD_q44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8xqHJDgOAVir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image = cv2.imread(\"gdrive/MyDrive/Furniture-Dataset/PepperFry_train/Combo Muddha XXXL Bean Bag _ Round Pouffe with Beans in Black _Pink Colour by Sattva._image_0.jpg\")\n",
        "\n",
        "# # #print(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
        "# print(image)\n",
        "#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ],
      "metadata": {
        "id": "O_ehf154FlbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import glob\n",
        "# for file_name in glob.glob(\"gdrive/MyDrive/Furniture-Dataset/PepperFry_SampleImages/*.jpg\"):\n",
        "#   #print(\"this\")\n",
        "#   image = cv2.imread(file_name)\n",
        "#   image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#   if image is None:\n",
        "#     print(file_name)"
      ],
      "metadata": {
        "id": "INezEOQOFyKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VUxG1cWBIcko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}